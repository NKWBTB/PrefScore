{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as M\n",
    "import evaluate as E\n",
    "import config as CFG\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\"billsum\", \"cnn_dailymail\", \"newsroom\", \"big_patent\", \"scientific_papers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hierarchical, loss_func, suffix='', separate=False):\n",
    "    for DATASET in DATASETS:\n",
    "        train_set = M.CustomDataset(os.path.join(CFG.DATASET_ROOT, DATASET, CFG.METHOD, 'train.tsv'), hierarchical=hierarchical)\n",
    "        print(len(train_set))\n",
    "\n",
    "        model = M.Siamese(separate)\n",
    "        model.to(CFG.DEVICE)\n",
    "\n",
    "        print(\"Training from\", DATASET)\n",
    "        M.train_model(model, train_set, loss_func=loss_func, shuffle=False)\n",
    "\n",
    "        CKPT_PATH = os.path.join(CFG.RESULT_ROOT, DATASET, CFG.METHOD+suffix, \"model.pth\")\n",
    "        if not os.path.exists(os.path.dirname(CKPT_PATH)):\n",
    "            os.makedirs(os.path.dirname(CKPT_PATH))\n",
    "\n",
    "        scorer = model.base_model\n",
    "        torch.save(scorer.state_dict(), CKPT_PATH)\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(suffix='', separate=False):\n",
    "    for DATASET in DATASETS:\n",
    "        CKPT_PATH = os.path.join(CFG.RESULT_ROOT, DATASET, CFG.METHOD+suffix, \"model.pth\")\n",
    "        scorer = M.Scorer(separate)\n",
    "        scorer.load_state_dict(torch.load(CKPT_PATH, map_location=CFG.DEVICE))\n",
    "        scorer.to(CFG.DEVICE)\n",
    "        scorer.eval()\n",
    "\n",
    "        E.evaluate_newsroom(\"human/newsroom/newsroom-human-eval.csv\", os.path.join(CFG.RESULT_ROOT, DATASET, CFG.METHOD+suffix, \"test_results_newsroom.tsv\"), scorer)\n",
    "        E.evaluate_realsumm(\"human/realsumm/realsumm_100.tsv\", os.path.join(CFG.RESULT_ROOT, DATASET, CFG.METHOD+suffix, \"test_results_realsumm.tsv\"), scorer)\n",
    "        E.evaluate_tac(\"human/tac/TAC2010_all.json\", os.path.join(CFG.RESULT_ROOT, DATASET, CFG.METHOD+suffix, \"test_results_tac.tsv\"), scorer)\n",
    "\n",
    "        del scorer\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchichal False\n",
      "64152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from billsum\n",
      "CrossEntropyLoss 0.0 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|â–Œ                                                                                                                                                                                       | 50/15000 [00:39<3:17:13,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "train(hierarchical=False, loss_func='CrossEntropyLoss', separate=False)\n",
    "test(separate=False)\n",
    "# train(hierarchical=False, loss_func='MarginRankingLoss', suffix='_NonHier', separate=True)\n",
    "# test(suffix='_NonHier', separate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f73e00d382cd9fa8ab9231eb38f700e3a55dd397749a3015a484e3d418ba51cb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
