{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import html\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scorer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scorer, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.model.config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, article, summary):\n",
    "        inputs = self.tokenizer(article, summary, padding='longest', truncation=\"longest_first\" , return_tensors='pt').to(device)\n",
    "        outputs = self.model(**inputs)\n",
    "        x = self.fc(outputs.pooler_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testm = Scorer()\n",
    "# outputs = testm.forward(['Today is a great day', 'Today is not a good day'], ['Good day', 'Bad day'])\n",
    "# print(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.base_model = Scorer()\n",
    "    \n",
    "    def forward(self, article, summary1, summary2):\n",
    "        out1 = self.base_model(article, summary1)\n",
    "        out2 = self.base_model(article, summary2)\n",
    "        out = torch.cat((out1, out2), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testm = Siamese()\n",
    "# outputs = testm.forward(['Today is a great day', 'Today is not a good day'], ['Good day', 'Bad day'], ['Bad day', 'Good day'])\n",
    "# print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET='billsum'\n",
    "# DATASET_ROOT= '../exp/data/'\n",
    "# METHOD = 'ordered_siam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datapath, nums=None):\n",
    "        self.data = []\n",
    "        with open(datapath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                elements = line.split('\\t')\n",
    "                size = len(elements)\n",
    "                for i in range(1, size-1):\n",
    "                    self.data.append([elements[0], elements[i], elements[i+1]])\n",
    "                # Limit the number of lines used\n",
    "                if nums is not None:\n",
    "                    nums -= 1\n",
    "                    if nums == 0:\n",
    "                        break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx][0], self.data[idx][1], self.data[idx][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = CustomDataset(os.path.join(DATASET_ROOT, DATASET, METHOD, 'train.tsv'))\n",
    "# print(len(train_set))\n",
    "# train_dataloader = DataLoader(train_set, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Siamese()\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_set, max_iter=10000, tune=False):\n",
    "    epochs = 1\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_dataloader = DataLoader(train_set, batch_size=12, shuffle=True)\n",
    "\n",
    "    while tune or epochs > 0:\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        with tqdm(total=min(len(train_dataloader), max_iter)) as pbar:\n",
    "            for j, (article, sum1, sum2) in enumerate(train_dataloader):\n",
    "                if j >= max_iter:\n",
    "                    break\n",
    "                output = model(article, sum1, sum2)\n",
    "                labels = torch.tensor([0]*len(article), dtype=torch.long).to(device)\n",
    "                loss = loss_fn(output, labels)\n",
    "\n",
    "                #accuracy\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                pbar.update(1)\n",
    "                if j % 100 == 99:\n",
    "                    # pbar.write(\"Iteration {}, Loss {}\".format(j+1, running_loss))\n",
    "                    running_loss = 0\n",
    "        \n",
    "        epochs -= 1\n",
    "        if tune:\n",
    "            model.eval()\n",
    "            acc = 0.0\n",
    "            for i in tqdm(range(len(train_set))):\n",
    "                article, sum1, sum2 = train_set[i]\n",
    "                output = model([article], [sum1], [sum2])\n",
    "                if output[0][0] > output[0][1]:\n",
    "                    acc += 1\n",
    "            \n",
    "            # Overfit on train set\n",
    "            acc /= len(train_set)\n",
    "            print(acc)\n",
    "            if acc > 0.9:\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CKPT_PATH = os.path.join(\"../exp/result_bert_base_uncased\", DATASET, METHOD, \"model.pth\")\n",
    "# if not os.path.exists(os.path.dirname(CKPT_PATH)):\n",
    "#     os.makedirs(os.path.dirname(CKPT_PATH))\n",
    "\n",
    "# scorer = model.base_model\n",
    "# torch.save(scorer.state_dict(), CKPT_PATH)\n",
    "# scorer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate for TAC2010\n",
    "def evaluate_tac(json_file, output_path, scorer):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        examples = []\n",
    "        tac = json.load(open(json_file, 'r', encoding=\"utf-8\"))\n",
    "        for docset in tac.keys():\n",
    "            for article in tac[docset][\"articles\"]: \n",
    "                # each of 10 articles is a list of strings \n",
    "                article = \" \".join(article)\n",
    "                article = article.replace(\"\\n\", \" \")\n",
    "                article = article.replace(\"\\t\", \" \")\n",
    "                if len(article) == 0:\n",
    "                    article = \" .\" \n",
    "\n",
    "                _doc = ' '.join(article.split()[0:400])\n",
    "\n",
    "                for summarizer in tac[docset][\"summaries\"].keys():\n",
    "                    summary = \" \".join(tac[docset]['summaries'][summarizer]['sentences']) \n",
    "                    # no need for [0] since we changed the format of jsonfile\n",
    "                    summary = summary.replace(\"\\n\", \" \")\n",
    "                    summary = summary.replace(\"\\t\", \" \")\n",
    "                    if len(summary) == 0:\n",
    "                        summary = \" .\"\n",
    "\n",
    "                    _sum = ' '.join(summary.split()[0:200])\n",
    "                    \n",
    "                    label = scorer([article], [summary]).detach().cpu().numpy()[0][0]\n",
    "                    f.write(str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_tac(\"TAC2010_all.json\", os.path.join(\"../exp/result_bert_base_uncased\", DATASET, METHOD, \"test_results_tac.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_newsroom(csv_file, output_path, scorer):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        with open(csv_file, \"r\", encoding=\"utf-8\") as csvfile: \n",
    "            reader = csv.reader(csvfile, delimiter=\",\", quotechar=\"\\\"\") \n",
    "            counter = 0 \n",
    "            for row in reader: \n",
    "                if counter > 0:\n",
    "                    [_doc, _sum] = row[2:4]\n",
    "                    _doc = _doc.replace(\"</p><p>\", \"\")\n",
    "                    _sum = _sum.replace(\"</p><p>\", \"\")\n",
    "                    _doc=html.unescape(_doc) \n",
    "                    _sum=html.unescape(_sum) \n",
    "\n",
    "                    label = scorer([_doc], [_sum]).detach().cpu().numpy()[0][0]\n",
    "                    f.write(str(label) + \"\\n\")\n",
    "                counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_newsroom(\"newsroom-human-eval.csv\", os.path.join(\"../exp/result_bert_base_uncased\", DATASET, METHOD, \"test_results_newsroom.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_realsumm(tsv_file, output_path, scorer):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        with open(tsv_file, \"r\", encoding=\"utf-8\") as tsv:\n",
    "            for line in tsv:\n",
    "                line = line.split('\\t')\n",
    "                _doc = ' '.join(line[0].split())\n",
    "                for j in range(1, len(line)) :\n",
    "                    _sum = ' '.join(line[j].split())\n",
    "                    \n",
    "                    label = scorer([_doc], [_sum]).detach().cpu().numpy()[0][0]\n",
    "                    f.write(str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_realsumm(\"realsumm_100.tsv\", os.path.join(\"../exp/result_bert_base_uncased\", DATASET, METHOD, \"test_results_realsumm.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summeval(tsv_file, output_path, scorer):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        with open(tsv_file, \"r\", encoding=\"utf-8\") as tsv:\n",
    "            for line in tsv:\n",
    "                line = line.split('\\t')\n",
    "                _doc = ' '.join(line[0].split())\n",
    "                _sum = ' '.join(line[1].split())\n",
    "                \n",
    "                label = scorer([_doc], [_sum]).detach().cpu().numpy()[0][0]\n",
    "                f.write(str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET=['billsum', 'scientific_papers', 'cnn_dailymail', 'big_patent'] #\n",
    "DATASET_ROOT= '../exp/data/'\n",
    "RESULT_ROOT = \"../exp/result_bert_base_uncased\"\n",
    "METHOD = 'ordered_siam_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset in DATASET:\n",
    "#     print(\"Loading {}\".format(dataset))\n",
    "#     train_set = CustomDataset(os.path.join(DATASET_ROOT, dataset, METHOD, 'train.tsv'))\n",
    "#     print(len(train_set))\n",
    "    \n",
    "#     model = Siamese()\n",
    "#     model.to(device)\n",
    "    \n",
    "#     print(\"Training...\")\n",
    "#     train_model(model, train_set)\n",
    "    \n",
    "#     CKPT_PATH = os.path.join(RESULT_ROOT, dataset, METHOD, \"model.pth\")\n",
    "#     if not os.path.exists(os.path.dirname(CKPT_PATH)):\n",
    "#         os.makedirs(os.path.dirname(CKPT_PATH))\n",
    "\n",
    "#     scorer = model.base_model\n",
    "#     torch.save(scorer.state_dict(), CKPT_PATH)\n",
    "#     scorer.eval()\n",
    "    \n",
    "#     print(\"Evaluating...\")\n",
    "#     evaluate_tac(\"TAC2010_all.json\", os.path.join(RESULT_ROOT, dataset, METHOD, \"test_results_tac.tsv\"), scorer)\n",
    "#     evaluate_newsroom(\"newsroom-human-eval.csv\", os.path.join(RESULT_ROOT, dataset, METHOD, \"test_results_newsroom.tsv\"), scorer)\n",
    "#     evaluate_realsumm(\"realsumm_100.tsv\", os.path.join(RESULT_ROOT, dataset, METHOD, \"test_results_realsumm.tsv\"), scorer)\n",
    "    \n",
    "#     del scorer\n",
    "#     del model\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASET:\n",
    "    CKPT_PATH = os.path.join(RESULT_ROOT, dataset, METHOD, \"model.pth\")\n",
    "    \n",
    "    model = Siamese()\n",
    "    model.base_model.load_state_dict(torch.load(CKPT_PATH))\n",
    "    model.to(device)\n",
    "    \n",
    "    scorer = model.base_model\n",
    "    scorer.eval()\n",
    "    \n",
    "    evaluate_summeval(\"summeval_100.tsv\", os.path.join(RESULT_ROOT, dataset, METHOD, \"test_results_summeval.tsv\"), scorer)\n",
    "    \n",
    "    del scorer\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tune & test on REALSUMM\n",
    "# samples = 2737\n",
    "\n",
    "# tune_path = os.path.join(RESULT_ROOT, 'cnn_dailymail', METHOD)\n",
    "\n",
    "# for nums in range(0, samples+1, samples):\n",
    "#     train_set = CustomDataset('realsumm_tune.tsv', nums)\n",
    "#     print(len(train_set))\n",
    "    \n",
    "#     model = Siamese()\n",
    "#     model.base_model.load_state_dict(torch.load(os.path.join(tune_path, 'model.pth')))\n",
    "#     model.to(device)\n",
    "    \n",
    "#     print(\"Tuning...\")\n",
    "#     if nums > 0:\n",
    "#         train_model(model, train_set, tune=True)\n",
    "    \n",
    "#     scorer = model.base_model\n",
    "#     scorer.eval()\n",
    "    \n",
    "#     evaluate_realsumm(\"realsumm_100.tsv\", os.path.join(tune_path, \"test_results_realsumm_{}.tsv\").format(nums), scorer)\n",
    "    \n",
    "#     del scorer\n",
    "#     del model\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "463b1de34f2100de6efd01b73525f1ec58c9e2681dc97479e65e4f4d50ef3d08"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
