# Correlation with human evaluation in [TAC2010](https://tac.nist.gov/data/past/2010/Summ10.html) Summarization Contest 

Files in this folder are related to computing the correlation between the score from our model and those in TAC2010 Summarization Contest. 
TAC2010 summarization contest is a multi-document summarization task. 
There are 44 topics. Each topic has two 10-document document sets, A and B. 
The 10 documents are converted into one summary by 4 out of 6 (7?) human (A to H) and 41 machine summarizers (3 to 43). There are also two baseline summarizers, Leadwords (ID=1) and MEAD (ID=2). 
We focus only document sets A for each topic because a summary for document sets B contains mainly incremental information from A to B. 

Summaries generated by the 4+41 summarizers are evaluated by Z human evaluators on several aspects. Here we focus on the last 3: the modified score, the linguistic quality, and the overall score. 
The score for a summary is the average of scores between it and all 10 document in the corresponding document set.

Request the dataset access [here](https://tac.nist.gov/data/past/2010/Summ10.html). Configure **tac_config.py** properly to run the codes.

# Files
* tac_config.py: path configurations for the dataset
* tac.py: converts document-summary pairs and scores in TAC result files into JSON format (**TAC2020_all.json**).
* baselines.py: computes the reference-based upper bounds (**baselines.json**). 
* ref-free-baselines.py: computes the reference-free baselines (**baselines_ref_free.json**).
* baselines_corr.py: computes the correlation between human evaluation and baselines scores (**rouge_score.tsv**) . 
* test_eval.py: computes the correlation between scores from our BERT model and human evaluation scores in TAC2010.
