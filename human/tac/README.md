# Correlation with human evaluation in TAC2010 Summarization Contest 

Files in this folder are related to computing the correlation between the score from our model and those in TAC2010 Summarization Contest. 
TAC2010 summarization contest is a multi-document summarization task. 
There are 44 topics. Each topic has two 10-document document sets, A and B. 
The 10 documents are converted into one summary by 4 out of 6 (7?) human (A to H) and 41 machine summarizers (3 to 43). There are also two baseline summarizers, Leadwords (ID=1) and MEAD (ID=2). 
We focus only document sets A for each topic because a summary for document sets B contains mainly incremental information from A to B. 

Summaries generated by the 4+41 summarizers are evaluated by Z human evaluators on several aspects. Here we focus on the last 3: the modified score, the linguistic quality, and the overall score. 
The score for a summary is the average of scores between it and all 10 document in the corresponding document set.

# Files
* tac.py: converts document-summary pairs and scores in TAC result files into JSON format.
* baselines.py: computes the reference-based upper bounds. 
* ref-free-baselines.py: computes the reference-free baselines.
* baselines_corr.py: computes the correlation between human evaluation and baselines scores. 
* test_eval.py: computes the correlation between scores from our BERT model and human evaluation scores in TAC2010.
